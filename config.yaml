# torchdtcc config.yaml

model:
  input_dim: 1
  num_layers: 3
  num_clusters: 3
  hidden_dims: [100, 50, 50]  # Simplified architecture with consistent dimensions
  dilation_rates: [1, 4, 16]  # These look good
  tau_I: 0.2  # Lower temperature for sharper instance contrasts
  tau_C: 0.2  # Lower temperature for cluster contrasts
  stable_svd: false
  # In the paper the authors describe the autoencoder as two 
  # independent instances, which implies they are not siamese networks.
  # However, in contrastive learning usually the networks are siamese.
  # that is why the option for weight sharing is implemented.
  weight_sharing: true

warmup:
  save_path: "pretrained_dtccae_{}.pth"
  learning_rate: 0.00005
  weight_decay: 0.1
  num_epochs: 10000
  gradient_clip: 0
  patience: 600
  lr_scheduler:
    type: "StepLR"
    step_size: 600
    gamma: 0.1
    

trainer:
  warmup: false
  from_pretrained:
    path: "" 
  save_path: "dtcc_model_{}.pth"
  learning_rate: 0.000001
  weight_decay: 0.1
  num_epochs: 10000
  gradient_clip: 0
  # DTCC specific parameters
  update_interval: 5
  lambda_cd: 2.0
  ablation: ["all"] # "contrastive", "all", "reconstruction", "distribution"

mlflow:
  server_uri: "databricks" # remove or set to databricks if using on databricks
  # databricks example. databricks requires this scheme for experiment name
  experiment_path: "/Users/first.last@companydomain.com/{}"
  experiment: "my_experiment_name"
  run: 'my_run_1'

data:
  path: "./data/plane/"
  batch_size: 64
  normalize: true 


output:
  soft_clusters: "soft_clusters.npy"
  hard_clusters_argmax: "hard_clusters_argmax.npy"
  hard_clusters_kmeans: "hard_clusters_kmeans.npy"

device: "cuda"  # or "cpu"